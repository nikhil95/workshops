{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDultjMLQRGT"
   },
   "source": [
    "# Import necessary Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "!pip install segyio\n",
    "!pip install segysak\n",
    "!pip install seaborn\n",
    "!pip install plotly\n",
    "!pip install scikit-learn\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4wK_1Sp5oZz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jufY6EwkRKUX"
   },
   "source": [
    "# Data Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boxKPJy4QohL"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/data/CSV_train.csv', sep=';')\n",
    "\n",
    "test = pd.read_csv('/data/CSV_test.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "id": "m81k7kmiRyV2",
    "outputId": "29d6083a-6b0d-4169-a32f-2c718197d609"
   },
   "outputs": [],
   "source": [
    "train.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_Lu1cjxUV34"
   },
   "source": [
    "# Exploratory Data Analysis & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the number of wells for training and testing in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5rUlCv9UVh_"
   },
   "outputs": [],
   "source": [
    "Alldata=pd.concat([train, test], axis=0).reset_index().rename(columns={'index': 'idx'})\n",
    "Alldata.loc[train.index, 'Dataset'] = 'Train'\n",
    "Alldata.loc[train.index.max()+1:, 'Dataset'] = 'Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ew_3QxNVUVfF",
    "outputId": "d7e2bf7d-7a1a-410c-8c2d-86c890190a79"
   },
   "outputs": [],
   "source": [
    "train_wells = Alldata['WELL'][Alldata.Dataset=='Train'].unique()\n",
    "print('No of train wells: %s' % len(train_wells))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGJuLb--UVai",
    "outputId": "fe77f574-38ed-4221-f14b-08e91783faec"
   },
   "outputs": [],
   "source": [
    "test_wells = Alldata['WELL'][Alldata.Dataset=='Test'].unique()\n",
    "print('No of test wells: %s' % len(test_wells))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a dictionary of the lithology names with respect to their unique number key present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-FFUVuVeUVPm",
    "outputId": "f5c81784-5f4e-47f7-811b-6c08b603204d"
   },
   "outputs": [],
   "source": [
    "lithology_keys = {} # initialize dictionary\n",
    "litho=['Sandstone', 'Shale', 'SandyShale', 'Limestone', 'Chalk', 'Dolomite',\n",
    "       'Marl', 'Anhydrite', 'Halite', 'Coal', 'Basement', 'Tuff']\n",
    "col = np.sort(Alldata['FORCE_2020_LITHOFACIES_LITHOLOGY'][Alldata.Dataset=='Train'].unique())\n",
    "for index, name in enumerate(col):\n",
    "    lithology_keys[name] = litho[index]\n",
    "lithology_keys = {30000: 'Sandstone',\n",
    "                 65030: 'Sandstone/Shale',\n",
    "                 65000: 'Shale',\n",
    "                 80000: 'Marl',\n",
    "                 74000: 'Dolomite',\n",
    "                 70000: 'Limestone',\n",
    "                 70032: 'Chalk',\n",
    "                 88000: 'Halite',\n",
    "                 86000: 'Anhydrite',\n",
    "                 99000: 'Tuff',\n",
    "                 90000: 'Coal',\n",
    "                 93000: 'Basement'}\n",
    "lithology_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the count of different facies labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 764
    },
    "id": "dqNn7V63VEHP",
    "outputId": "6875e521-421a-43ea-df84-bb8abe219a30"
   },
   "outputs": [],
   "source": [
    "counts = train['FORCE_2020_LITHOFACIES_LITHOLOGY'].value_counts()\n",
    "names = []\n",
    "percentage = []\n",
    "N = train['FORCE_2020_LITHOFACIES_LITHOLOGY'].shape[0]\n",
    "for item in counts.items():\n",
    "    names.append(lithology_keys[item[0]])\n",
    "    percentage.append(float(item[1])/N*100)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 7))\n",
    "ax.bar(x=np.arange(len(names)), height=percentage)\n",
    "ax.set_xticklabels(names, rotation=45)\n",
    "ax.set_xticks(np.arange(len(names)))\n",
    "ax.set_ylabel('Lithology presence (\\%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of feature names to be used for training the ML model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3E_25TTVEEM"
   },
   "outputs": [],
   "source": [
    "feature_names = ['DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'CALI', 'GR', 'RSHA', 'RMED', 'RDEP', 'RHOB',\n",
    "                 'NPHI', 'PEF', 'DTC', 'DTS', 'SP', 'ROP', 'BS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a new dictionary of sequential facies numbering with respect to the unique lithology numbers in the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "cBqknAOtVD-k",
    "outputId": "ab26a885-8211-4201-91da-e3e95b6ad378"
   },
   "outputs": [],
   "source": [
    "Y = Alldata['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "lithology_numbers = {} # initialize dictionary\n",
    "lithology_numbers = {30000: 0,\n",
    "                     65030: 1,\n",
    "                     65000: 2,\n",
    "                     80000: 3,\n",
    "                     74000: 4,\n",
    "                     70000: 5,\n",
    "                     70032: 6,\n",
    "                     88000: 7,\n",
    "                     86000: 8,\n",
    "                     99000: 9,\n",
    "                     90000: 10,\n",
    "                     93000: 11}\n",
    "display(lithology_keys)\n",
    "Y = Y.map(lithology_numbers)\n",
    "Y.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store well labels and depths for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qii-D-1UVpYU"
   },
   "outputs": [],
   "source": [
    "well = Alldata['WELL']\n",
    "dataset = Alldata[['idx', 'Dataset']]\n",
    "depth = Alldata['DEPTH_MD']\n",
    "Strat = Alldata['GROUP']\n",
    "Formation = Alldata['FORMATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbzXAoevVpTN"
   },
   "outputs": [],
   "source": [
    "X_all = Alldata.drop(columns=['WELL', 'FORCE_2020_LITHOFACIES_LITHOLOGY', 'FORCE_2020_LITHOFACIES_CONFIDENCE',\n",
    "                              'idx', 'Dataset'])\n",
    "X=X_all[feature_names]\n",
    "\n",
    "Y=Y.reindex(X.index)\n",
    "Strat=Strat.reindex(X.index)\n",
    "Formation=Formation.reindex(X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill null values with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "jkLx88saVpPv",
    "outputId": "42ae5421-3483-47fd-c133-f25ea0a26bde"
   },
   "outputs": [],
   "source": [
    "X_imp=X.fillna(0)\n",
    "X_imp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rq7GDns9XHkA"
   },
   "source": [
    "## Log plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary of unique colours for each facies label, for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facies_names = lithology_keys.values()\n",
    "facies_colors = ['darkorange', '#228B22', 'grey', 'cyan', 'gold', 'lightseagreen',\n",
    "                 'lawngreen', 'lightblue', 'tan', '#FF4500', '#000000', 'magenta']\n",
    "#\n",
    "#facies_color_map is a dictionary that maps facies labels to their respective colors\n",
    "facies_color_map = {}\n",
    "for ind, label in enumerate(facies_names):\n",
    "    facies_color_map[label] = facies_colors[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xq02mzeIUVli"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.colors as colors\n",
    "def make_facies_log_plot(logs, curves, well_name, facies_colors):\n",
    "\n",
    "    #make sure logs are sorted by depth\n",
    "    cmap_facies = colors.ListedColormap(facies_colors, 'indexed')\n",
    "\n",
    "    colours=['k','b','r','g','m','c','lime','gold','sienna']\n",
    "\n",
    "    ztop=logs.Depth.min(); zbot=logs.Depth.max()\n",
    "\n",
    "    cluster=np.repeat(logs['litho_real'].values.reshape(-1, 1),50,axis=1)\n",
    "\n",
    "    num_curves = len(curves)\n",
    "    f, ax = plt.subplots(nrows=1, ncols=num_curves+1, figsize=(num_curves*2, 12))\n",
    "\n",
    "    for ic, col in enumerate(curves):\n",
    "\n",
    "        # if the curve doesn't exist, make it zeros\n",
    "        if np.all(np.isnan(logs[col])):\n",
    "            curve = np.empty(logs[col].values.shape)\n",
    "            curve[:] = np.nan\n",
    "\n",
    "        else:\n",
    "            curve = logs[col]\n",
    "\n",
    "        ax[ic].plot(curve, logs.Depth,colours[ic])\n",
    "        ax[ic].set_xlabel(col)\n",
    "        if ic != 0:\n",
    "            ax[ic].set_yticklabels([]);\n",
    "\n",
    "    # make the lithfacies column\n",
    "    im=ax[num_curves].imshow(cluster, interpolation='none', aspect='auto',\n",
    "                    cmap=cmap_facies,vmin=0,vmax=len(facies_colors)-1)\n",
    "\n",
    "    divider = make_axes_locatable(ax[num_curves])\n",
    "    cax = divider.append_axes(\"right\", size=\"20%\", pad=0.05)\n",
    "    cbar=plt.colorbar(im, cax=cax)\n",
    "    cbar.set_label((13*' ').join(['  SS', 'SS-Sh', 'Sh',\n",
    "                                ' Marl', 'Dol', 'Lims', 'Chlk ',\n",
    "                                '  Hal', 'Anhy', 'Tuf', 'Coal', 'Bsmt']))\n",
    "    cbar.set_ticks(range(0,1)); cbar.set_ticklabels('')\n",
    "\n",
    "    for i in range(len(ax)-1):\n",
    "        ax[i].set_ylim(ztop,zbot)\n",
    "        ax[i].invert_yaxis()\n",
    "        ax[i].grid()\n",
    "        ax[i].locator_params(axis='x', nbins=3)\n",
    "\n",
    "    ax[0].set_ylabel(\"DEPTH\")\n",
    "    ax[num_curves].set_xlabel('Lithology')\n",
    "    ax[num_curves].set_yticklabels([])\n",
    "    ax[num_curves].set_xticklabels([])\n",
    "\n",
    "    f.suptitle('Well: %s'% well_name, fontsize=14,y=0.94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0h41PlrOVpML",
    "outputId": "79b0a165-8ef6-4147-ac5c-a4ed7a65abf4"
   },
   "outputs": [],
   "source": [
    "well_no=25\n",
    "logs = pd.concat([X_imp,\n",
    "                  Y], axis=1).rename(columns = {'FORCE_2020_LITHOFACIES_LITHOLOGY':'litho_real', 'DEPTH_MD':'Depth'\n",
    "                                               })\n",
    "make_facies_log_plot(logs[well==train_wells[well_no]], ['CALI', 'GR', 'RHOB', 'NPHI', 'RSHA', 'RDEP', 'PEF'],\n",
    "                     train_wells[well_no],\n",
    "                     facies_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HuNoKob8RUG7"
   },
   "source": [
    "# Preprocessing and Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1HHu00J6gBZ"
   },
   "outputs": [],
   "source": [
    "def drop_columns(data, *args):\n",
    "\n",
    "    '''\n",
    "    function used to drop columns.\n",
    "    args::\n",
    "      data:  dataframe to be operated on\n",
    "      *args: a list of columns to be dropped from the dataframe\n",
    "\n",
    "    return: returns a dataframe with the columns dropped\n",
    "    '''\n",
    "\n",
    "    columns = []\n",
    "    for _ in args:\n",
    "        columns.append(_)\n",
    "\n",
    "    data = data.drop(columns, axis=1)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WC3b0YpJD1kC"
   },
   "outputs": [],
   "source": [
    "def process(data):\n",
    "\n",
    "    '''\n",
    "    function to process dataframe by replacing missing, infinity values with -999\n",
    "\n",
    "    args::\n",
    "      data:  dataframe to be operated on\n",
    "\n",
    "    returns dataframe with replaced values\n",
    "    '''\n",
    "\n",
    "    cols = list(data.columns)\n",
    "    for _ in cols:\n",
    "\n",
    "        data[_] = np.where(data[_] == np.inf, -999, data[_])\n",
    "        data[_] = np.where(data[_] == np.nan, -999, data[_])\n",
    "        data[_] = np.where(data[_] == -np.inf, -999, data[_])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZiWPAQuRod2"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eAUzxMPD1eA"
   },
   "outputs": [],
   "source": [
    "# Feature windows concatenation function\n",
    "def augment_features_window(X, N_neig):\n",
    "\n",
    "    # Parameters\n",
    "    N_row = X.shape[0]\n",
    "    N_feat = X.shape[1]\n",
    "\n",
    "    # Zero padding\n",
    "    X = np.vstack((np.zeros((N_neig, N_feat)), X, (np.zeros((N_neig, N_feat)))))\n",
    "\n",
    "    # Loop over windows\n",
    "    X_aug = np.zeros((N_row, N_feat*(2*N_neig+1)))\n",
    "    for r in np.arange(N_row)+N_neig:\n",
    "        this_row = []\n",
    "        for c in np.arange(-N_neig,N_neig+1):\n",
    "            this_row = np.hstack((this_row, X[r+c]))\n",
    "        X_aug[r-N_neig] = this_row\n",
    "\n",
    "    return X_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0ExSV_UD1bf"
   },
   "outputs": [],
   "source": [
    "# Feature gradient computation function\n",
    "def augment_features_gradient(X, depth):\n",
    "\n",
    "    # Compute features gradient\n",
    "    d_diff = np.diff(depth).reshape((-1, 1))\n",
    "    d_diff[d_diff==0] = 0.001\n",
    "    X_diff = np.diff(X, axis=0)\n",
    "    X_grad = X_diff / d_diff\n",
    "\n",
    "    # Compensate for last missing value\n",
    "    X_grad = np.concatenate((X_grad, np.zeros((1, X_grad.shape[1]))))\n",
    "\n",
    "    return X_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SPvO1OKEEWW"
   },
   "outputs": [],
   "source": [
    "# Combined feature augmentation function\n",
    "def augment_features(X, well, depth, N_neig=1):\n",
    "\n",
    "    # Augment features\n",
    "    X_aug = np.zeros((X.shape[0], X.shape[1]*(N_neig*2+2)))\n",
    "    for w in np.unique(well):\n",
    "        w_idx = np.where(well == w)[0]\n",
    "        X_aug_win = augment_features_window(X[w_idx, :], N_neig)\n",
    "        X_aug_grad = augment_features_gradient(X[w_idx, :], depth[w_idx])\n",
    "        X_aug[w_idx, :] = np.concatenate((X_aug_win, X_aug_grad), axis=1)\n",
    "\n",
    "    # Find padded rows\n",
    "    padded_rows = np.unique(np.where(X_aug[:, 0:7] == np.zeros((1, 7)))[0])\n",
    "\n",
    "    return X_aug, padded_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UexjHd7SHYR"
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Quv4iavk2nXk"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def show_evaluation(predictions, true_labels):\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f'Accuracy is: {accuracy}')\n",
    "\n",
    "    # Generate and print classification report\n",
    "    class_report = classification_report(true_labels, predictions)\n",
    "    print(f'Classification Report:\\n{class_report}')\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQSZM0glXhPD"
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "\n",
    "    '''\n",
    "    class to lithology prediction\n",
    "    '''\n",
    "\n",
    "    def __init__(self, train, test):\n",
    "\n",
    "        '''\n",
    "        takes in the train and test dataframes\n",
    "        '''\n",
    "\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "\n",
    "\n",
    "    def __call__(self, plot = False):\n",
    "\n",
    "      return self.fit(plot)\n",
    "\n",
    "    def preprocess(self, train, test):\n",
    "\n",
    "        '''\n",
    "        method to prepare datasets for training and predictions\n",
    "        accepts both the train and test dataframes as arguments\n",
    "\n",
    "        returns the prepared train, test datasets along with the\n",
    "        lithology labels and numbers.\n",
    "\n",
    "        '''\n",
    "\n",
    "        #concatenating both train and test datasets for easier and uniform processing\n",
    "\n",
    "        ntrain = train.shape[0]\n",
    "        ntest = test.shape[0]\n",
    "        target = train.FORCE_2020_LITHOFACIES_LITHOLOGY.copy()\n",
    "        df = pd.concat((train, test)).reset_index(drop=True)\n",
    "\n",
    "        #mapping the lithology labels to ordinal values for better modelling\n",
    "\n",
    "        lithology = train['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "\n",
    "        lithology_numbers = {30000: 0,\n",
    "                        65030: 1,\n",
    "                        65000: 2,\n",
    "                        80000: 3,\n",
    "                        74000: 4,\n",
    "                        70000: 5,\n",
    "                        70032: 6,\n",
    "                        88000: 7,\n",
    "                        86000: 8,\n",
    "                        99000: 9,\n",
    "                        90000: 10,\n",
    "                        93000: 11}\n",
    "\n",
    "        lithology1 = lithology.map(lithology_numbers)\n",
    "\n",
    "        #implementing Bestagini's augmentation procedure\n",
    "\n",
    "        train_well = train.WELL.values\n",
    "        train_depth = train.DEPTH_MD.values\n",
    "\n",
    "        test_well = test.WELL.values\n",
    "        test_depth = test.DEPTH_MD.values\n",
    "\n",
    "\n",
    "\n",
    "        print(f'shape of concatenated dataframe before dropping columns {df.shape}')\n",
    "\n",
    "        cols = ['FORCE_2020_LITHOFACIES_CONFIDENCE', 'SGR', 'DTS', 'RXO', 'ROPA'] #columns to be dropped\n",
    "        df = drop_columns(df, *cols)\n",
    "        print(f'shape of dataframe after dropping columns {df.shape}')\n",
    "        print(f'{cols} were dropped')\n",
    "\n",
    "        #Label encoding the GROUP, FORMATION and WELLS features as these improved the performance of the models on validations\n",
    "\n",
    "        df['GROUP_encoded'] = df['GROUP'].astype('category')\n",
    "        df['GROUP_encoded'] = df['GROUP_encoded'].cat.codes\n",
    "        df['FORMATION_encoded'] = df['FORMATION'].astype('category')\n",
    "        df['FORMATION_encoded'] = df['FORMATION_encoded'].cat.codes\n",
    "        df['WELL_encoded'] = df['WELL'].astype('category')\n",
    "        df['WELL_encoded'] = df['WELL_encoded'].cat.codes\n",
    "        print(f'shape of dataframe after label encoding columns {df.shape}')\n",
    "\n",
    "\n",
    "        #FURTHER PREPRATION TO SPLIT DATAFRAME INTO TRAIN AND TEST DATASETS AFTER PREPRATION\n",
    "        print(f'Splitting concatenated dataframe into training and test datasets...')\n",
    "        df = df.drop(['WELL', 'GROUP', 'FORMATION'], axis=1)\n",
    "        print(df.shape)\n",
    "\n",
    "        df = df.fillna(-999)\n",
    "        df = process(df)\n",
    "        data = df.copy()\n",
    "\n",
    "        train2 = data[:ntrain].copy()\n",
    "        train2.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
    "\n",
    "        test2 = data[ntrain:(ntest+ntrain)].copy()\n",
    "        test2.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
    "        test2 = test2.reset_index(drop=True)\n",
    "\n",
    "        traindata = train2\n",
    "        testdata = test2\n",
    "\n",
    "        print(f'Shape of train and test datasets before augmentation {traindata.shape, testdata.shape}')\n",
    "\n",
    "        traindata1, padded_rows = augment_features(pd.DataFrame(traindata).values, train_well, train_depth)\n",
    "        testdata1, padded_rows = augment_features(pd.DataFrame(testdata).values, test_well, test_depth)\n",
    "\n",
    "        print(f'Shape of train and test datasets after augmentation {traindata1.shape, testdata1.shape}')\n",
    "\n",
    "        return traindata1, testdata1, lithology1, lithology_numbers\n",
    "\n",
    "    # ___________________________________________________________________________________________________________________________________________________________\n",
    "    def fit(self, plot):\n",
    "\n",
    "      '''\n",
    "      method to train model and make predictions\n",
    "\n",
    "      returns the test predictions, trained model, and lithology numbers used for making the submission file\n",
    "      '''\n",
    "\n",
    "      traindata1, testdata1, lithology1, lithology_numbers = self.preprocess(self.train, self.test)\n",
    "\n",
    "      #using a 10-fold stratified cross-validation technique and seting the shuffle parameter to true\n",
    "      #as this improved the validation performance better\n",
    "\n",
    "      split = 10\n",
    "      kf = StratifiedKFold(n_splits=split, shuffle=True)\n",
    "\n",
    "      open_test = np.zeros((len(testdata1), 12))\n",
    "\n",
    "      #100 n-estimators and 10 max-depth\n",
    "      model = XGBClassifier(n_estimators=100, max_depth=10, booster='gbtree',\n",
    "                            objective='multi:softprob', learning_rate=0.1, random_state=0,\n",
    "                            subsample=0.9, colsample_bytree=0.9,\n",
    "                            eval_metric='mlogloss', verbose=2020, reg_lambda=1500)\n",
    "\n",
    "\n",
    "      i = 1\n",
    "      for (train_index, test_index) in kf.split(pd.DataFrame(traindata1), pd.DataFrame(lithology1)):\n",
    "        X_train, X_test = pd.DataFrame(traindata1).iloc[train_index], pd.DataFrame(traindata1).iloc[test_index]\n",
    "        Y_train, Y_test = pd.DataFrame(lithology1).iloc[train_index],pd.DataFrame(lithology1).iloc[test_index]\n",
    "\n",
    "        model.fit(X_train, Y_train, early_stopping_rounds=100, eval_set=[(X_test, Y_test)], verbose=100)\n",
    "        prediction = model.predict(X_test)\n",
    "        print(show_evaluation(prediction, Y_test))\n",
    "\n",
    "        print(f'-----------------------FOLD {i}---------------------')\n",
    "        i+=1\n",
    "\n",
    "        open_test += model.predict_proba(pd.DataFrame(testdata1))\n",
    "\n",
    "      open_test= pd.DataFrame(open_test/split)\n",
    "\n",
    "      open_test = np.array(pd.DataFrame(open_test).idxmax(axis=1))\n",
    "\n",
    "      print('---------------CROSS VALIDATION COMPLETE')\n",
    "      print('----------------TEST EVALUATION------------------')\n",
    "\n",
    "\n",
    "      return open_test, model, lithology_numbers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hea5pu_YTvjY"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "t3dOrluJWtPM",
    "outputId": "49671e7c-c36e-48a1-bef6-e31216f0d7e4"
   },
   "outputs": [],
   "source": [
    "func_= Model(train, test)\n",
    "prediction, model, redundant = func_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_well = train[train['WELL'] == '15/9-17']\n",
    "\n",
    "features = selected_well.drop(columns=['GROUP', 'FORMATION'])\n",
    "\n",
    "processed_features = func_.preprocess(features, test)[0]\n",
    "\n",
    "well_predictions = model.predict(processed_features)\n",
    "\n",
    "well_predictions_labels = [[key for key, value in redundant.items() if value == label] for label in well_predictions]\n",
    "\n",
    "# print(\"Predicted Lithology Labels for all instances in the well:\", well_predictions_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = selected_well['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "series = pd.Series([item for sublist in well_predictions_labels for item in sublist])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "facies_mapping = {\n",
    "    30000: 'Sandstone',\n",
    "    65030: 'Sandstone/Shale',\n",
    "    65000: 'Shale',\n",
    "    80000: 'Marl',\n",
    "    74000: 'Dolomite',\n",
    "    70000: 'Limestone',\n",
    "    70032: 'Chalk',\n",
    "    88000: 'Halite',\n",
    "    86000: 'Anhydrite',\n",
    "    99000: 'Tuff',\n",
    "    90000: 'Coal',\n",
    "    93000: 'Basement'\n",
    "}\n",
    "\n",
    "facies_colors = {\n",
    "    'Sandstone': 'red',\n",
    "    'Sandstone/Shale': 'blue',\n",
    "    'Shale': 'green',\n",
    "    'Marl': 'yellow',\n",
    "    'Dolomite': 'purple',\n",
    "    'Limestone': 'orange',\n",
    "    'Chalk': 'pink',\n",
    "    'Halite': 'brown',\n",
    "    'Anhydrite': 'gray',\n",
    "    'Tuff': 'cyan',\n",
    "    'Coal': 'black',\n",
    "    'Basement': 'magenta'\n",
    "}\n",
    "\n",
    "\n",
    "colors_original = [facies_colors[facies_mapping[facies]] for facies in original]\n",
    "colors_series = [facies_colors[facies_mapping[facies]] for facies in series]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
    "\n",
    "scatter1 = ax1.scatter(range(len(original)), [1] * len(original), c=colors_original, marker='|', s=1000)\n",
    "ax1.set_yticks([])\n",
    "ax1.set_title('Original Data')\n",
    "scatter2 = ax2.scatter(range(len(series)), [1] * len(series), c=colors_series, marker='|', s=1000)\n",
    "ax2.set_xlabel('Data Point')\n",
    "ax2.set_yticks([])\n",
    "ax2.set_title('Prediction')\n",
    "legend_labels = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=label) for label, color in facies_colors.items()]\n",
    "fig.legend(handles=legend_labels,  loc='center right',ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
